{"cells":[{"cell_type":"markdown","source":["-sandbox\n### Spotify Music Recommendation System by Hongyang (Bruce) Yang"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### 1. Load and display the data"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM  tracks_csv"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sql\n\nSELECT count(*) FROM  tracks_csv"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\nSELECT * FROM  music_csv"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql\n\nSELECT count(*) FROM  music_csv"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql\nSELECT count(Distinct TrackID) FROM  music_csv"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\nSELECT * FROM cust_csv"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql\n\nSELECT count(*) FROM  cust_csv"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sql\nSELECT count(Distinct CustID) FROM  cust_csv"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["trackDF = sqlContext.read.format(\"csv\").options(header='true', inferSchema='true').load(\"/FileStore/tables/tracks.csv\")\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["trackDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(trackDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["musicDF = sqlContext.read.format(\"csv\").options(header='true', inferSchema='true').load(\"/FileStore/tables/music.csv\")\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["musicDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(musicDF)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["customerDF = sqlContext.read.format(\"csv\").options(header='true', inferSchema='true').load(\"/FileStore/tables/cust.csv\")\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["customerDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(customerDF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["-sandbox\n### 2. Exploratory Data Analysis in SQL"],"metadata":{}},{"cell_type":"code","source":["CustName = customerDF.select(\"*\").withColumnRenamed(\"CustID\",'CCustID')\ndisplay(CustName)\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["MusicName = musicDF.select(\"*\").withColumnRenamed(\"TrackID\",'MTrackID')\ndisplay(MusicName)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql.functions import col\ntrack_custJoin=trackDF.join(CustName, trackDF.CustID == CustName.CCustID, \"left_outer\")\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["track_allJoin = track_custJoin.join(MusicName,track_custJoin.TrackId == MusicName.MTrackID,\"left_outer\")\n\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(track_allJoin)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["track_allJoin.createOrReplaceTempView(\"track_allJoin\")\n"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\nSELECT CustID, Name, Count(DISTINCT TrackID) AS NumOfTrack \nFROM track_allJoin \nGroup By CustID, Name\nOrder BY NumOfTrack DESC\n"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["-sandbox\nThat means `Gregory Koval` (CustID = 0) listened a total of 1617 out of 1735 tracks.\n\nHe almost finished all the tracks. He really loves music!"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(Distinct TrackID) FROM track_allJoin\nWHERE CustID = 0"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql\nSELECT CustID, Name,count(TrackID) as mobileCount\nFROM track_allJoin\nWHERE mobile = 0\nGROUP BY CustID,Name\nORDER BY mobileCount DESC"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["-sandbox\nPaula Peltier really likes listening music using mobile device"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT TrackID, Title, Count(DISTINCT CustID ) AS NumOfCust\nFROM track_allJoin \nGroup By TrackID, Title \nOrder BY NumOfCust DESC\n\n"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["-sandbox\nThat means `Caught Up in You` (TrackId = 0) is the most popular music, there are 4190 out of 5000 customers ever listened to this track"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT DateTime FROM track_allJoin\nWHERE CustID = 0"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#unix_timestamp(DateTime,\"MM/dd/yyyy HH:mm:ss\").cast(\"double\").cast(\"timestamp\")\n"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%sql\nSELECT CustID, Name, TrackId, Title, Length,Mobile, split(DateTime, \" \")[0] as DateListen, split(DateTime, \" \")[1] as TimeListen \nFROM track_allJoin"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql.functions import split\n\nsplit_datetime = split(track_allJoin['DateTime'], ' ')\ntrack_allJoin = track_allJoin.withColumn('DateL', split_datetime.getItem(0))\ntrack_allJoin = track_allJoin.withColumn('TimeL', split_datetime.getItem(1))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display(track_allJoin)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["track_allJoin.createOrReplaceTempView(\"track_allJoin\")\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["final_table = sqlContext.sql(\"\"\"SELECT CustId, Name, TrackId, Title, Length, Mobile, Gender, DateL, TimeL FROM track_allJoin\"\"\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%sql\nselect count(*) as totalRows, count(distinct CustID) AS uniqueCustomer, count(distinct TrackID) as uniqueTrack\nfrom final_table"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["display(final_table)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, to_timestamp,to_utc_timestamp,hour\nfinal_table=final_table.withColumn(\"DateL\", to_date(\"DateL\", \"MM/dd/yy\"))\nfinal_table=final_table.withColumn(\"HourL\", hour(\"TimeL\"))\n\n\n\nfinal_table.createOrReplaceTempView(\"final_table\")\n"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["display(final_table)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["%sql\nselect HourL,count(TrackId) as TotalTrack \nfrom final_table\nGROup by HourL\nOrder by TotalTrack"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["-sandbox\n\nIt turns out that the customers are more likely to listen to the music at night before bedtime"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import DataFrameWriter\n\nresult_writer = DataFrameWriter(final_table)\nresult_writer.saveAsTable('final_table',format='parquet', mode='overwrite',path='/path/to/new/data/files')\n"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["-sandbox\n### 3. Study Implicit Rating"],"metadata":{}},{"cell_type":"code","source":["final_table.createOrReplaceTempView(\"final_table\")\n"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["%sql\nSELECT count(Name)\nFROM final_table\nWHERE Name IS NULL;"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["%sql\nSELECT TrackId,Title,count(*)\nFROM final_table\nWHERE CustId=0\nGroup by TrackId, Title\nOrder by TrackId"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%sql\nselect count(distinct TrackId) from final_table"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["%sql\n\nSELECT * from(\nSELECT TrackId, Length, rank() over (partition by TrackId order by count(*) desc) as rank\nFROM final_table\nWHERE Length>0\nGroup by TrackId, Length\norder by rank ) r\nwhere r.rank=1\n"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%sql\nselect percentile(cnt,0.25) as cnt_25,\n       percentile(cnt,0.5) as cnt_50,\n       percentile(cnt,0.75) as cnt_75,\n       percentile(cnt,0.9) as cnt_90,\n       percentile(cnt,0.95) as cnt_95,\n       percentile(cnt,0.99) as cnt_99,\n       percentile(cnt,0.995) as cnt_995,\n       percentile(cnt,0.999) as cnt_999\nfrom (\n  select TrackId, count(*) as cnt\n  from final_table\n  group by TrackId\n  )"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["-sandbox\na song has been listened 4009 times, this number falls in the 99.5% of all song played\n\nthe median play time is 422 times"],"metadata":{}},{"cell_type":"code","source":["%sql\n select TrackId, count(*) as cnt\n from final_table\n group by TrackId"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%sql\n select distinct DateL\n from final_table\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["%sql\nselect weekofyear(DateL),min(DateL), max(DateL), count(*)\nfrom final_table\ngroup by weekofyear(DateL)\norder by weekofyear(DateL)\n--week 13-18 are whole weeks"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["%sql\nselect CustId, TrackID, count(*),sum(case when Length>=200 then 1 else 0 end) as cnt\nfrom final_table\ngroup by CustId, TrackID\nOrder by CustId\n"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["%sql\nselect CustId, TrackID, count(*)\nfrom final_table\ngroup by CustId, TrackID\n"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%sql --cumulative play times\nselect percentile(cnt,0.25) as cnt_25, percentile(cnt,0.5) as cnt_50, percentile(cnt,0.75) as cnt_75, percentile(cnt,0.90) as cnt_90, percentile(cnt,0.925) as cnt_93, percentile(cnt,0.95) as cnt_95, percentile(cnt,0.975) as cnt_975, percentile(cnt,0.99) as cnt_99, percentile(cnt,0.999) as cnt_999\nfrom (\n  select CustId, TrackID, count(*),sum(case when Length>=0 then 1 else 0 end) as cnt\n  from final_table\n  group by CustId, TrackID\n  Order by CustId\n  )"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["-sandbox\n### 4. Study Explicit Rating"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nselect CustId, TrackID, \n       case when sum(case when Length>=0 then 1 else 0 end)<2 then 0\n            when sum(case when Length>=0 then 1 else 0 end)<4 then 1\n            when sum(case when Length>=0 then 1 else 0 end)<7 then 2\n            else 3 end as score\nfrom final_table\ngroup by CustId, TrackID\n"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["%sql\nselect score,count(*) \nfrom (\nselect CustId, TrackID, \n       case when sum(case when Length>=0 then 1 else 0 end)<2 then 0\n            when sum(case when Length>=0 then 1 else 0 end)<4 then 1\n            when sum(case when Length>=0 then 1 else 0 end)<7 then 2\n            else 3 end as score\nfrom final_table\ngroup by CustId, TrackID\n  )\ngroup by score\norder by score"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["-sandbox\n## 5. Build Popularity-based Benchmark model and Machine Learning model (ALS)"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n`https://www.ercim.eu/publication/ws-proceedings/DELOS5/nichols.pdf`\n\nWe believe an ideal solution is to improve the user interface to acquire implicit ratings by\nwatching user behaviors. Implicit ratings include measures of interest such as whether the user read\nan article and, if so, how much time the user spent reading it.\n\nThe main motivation for using implicit ratings is that it removes the cost to the evaluator of examining and\nrating the item.\n\n Each implicit rating will probably contain less 'value' than\nan explicit rating but the appropriate cost-benefit trade-off for different types of implicit data will have to be\ndetermined empirically\n\nthree types of implicit data: read/ignored, saved/deleted and replied/not replied"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## 5.1 Recommendation Model from Implicit Ratings: Train-Week 40-51, Test-Week 52 and 53"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### 'rating' is Customer #i has listened to the Song #j in a total of n times over the period of t, which n is the raing"],"metadata":{}},{"cell_type":"code","source":["#set training data using week 40 to week 51\nrating_im_train=spark.sql(\"\"\"\n\n  select CustId, TrackID, sum(case when Length>0 then 1 else 0 end) as rating\n  from final_table\n  where weekofyear(DateL)<=51 and weekofyear(DateL)>=40\n  group by CustId, TrackID\n\n\"\"\")\n\nrating_im_train.createOrReplaceTempView(\"rating_im_train\")\n"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["display(rating_im_train)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["#set testing data using week 51 and 52\nrating_im_test=spark.sql(\"\"\"\n\n  select CustId, TrackID, sum(case when Length>0 then 1 else 0 end) as rating\n  from final_table\n  where weekofyear(DateL)>51 \n  group by CustId, TrackID\n\n\"\"\")\nrating_im_test.createOrReplaceTempView(\"rating_im_test\")\n"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["display(rating_im_test)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["-sandbox\n1. CustId #1060 has listened to song #0 in a total of 1 time in the test-set period, so rating is 1\n2. CustId #69 has listened to song #1 in a total of 3 times in the test-set period, so rating is 3"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect count(*) from rating_im_train"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["%sql\nselect count(*) from rating_im_test"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["%sql\nselect * from rating_im_train\norder by TrackID\nlimit 20"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["%sql \n select TrackID, count(distinct CustId) as prediction\n from rating_im_train\n group by TrackID"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["model = spark.sql(\"\"\"\n      select TrackID, count(*) as prediction\n      from rating_im_train\n      group by TrackID\n    \"\"\")   \nmodel.createOrReplaceTempView(\"model\")\n"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["-sandbox\n\n## 'prediction' is the number of unique customers that have listened to the same track"],"metadata":{}},{"cell_type":"code","source":["display(model)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["\npredictions = spark.sql(\"\"\"\n      select t.*, m.prediction\n      from rating_im_test t left join\n           model m\n           on t.TrackId=m.TrackId\n    \"\"\")   \npredictions.createOrReplaceTempView(\"predictions\")\n"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["display(predictions)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["%sql\nselect * from predictions\nwhere CustID = 148"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["%sql \nselect CustId, TrackID, rating, \n      (rank() over (partition by CustId order by prediction desc)-1)*1.0/(count(TrackID) over (partition by CustId)-1) as p_rank\nfrom predictions"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":["-sandbox\n## 'p_rank' the percentile-ranking of program i within the ordered list of all programs prepared for user u."],"metadata":{}},{"cell_type":"code","source":["%sql \nselect CustId, TrackID, rating, prediction,\n      (rank() over (partition by CustId order by prediction desc)-1) as numerator, (count(TrackID) over (partition by CustId)-1) as denominator\nfrom predictions"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["-sandbox\n\n1. (count(TrackID) over (partition by CustId)-1) as denominator => CustId #148 has listened a total of 46 unique tracks\n2. minus 1 because we need 0% to 100%, so (numerator-1 )/ (denominator -1)\n3. (rank() over (partition by CustId order by prediction desc)-1) as numerator => create rank based on prediction (count total unique customers group by song)\n4. for example, CustId #148 has ever listend to 46 tracks in total, of these 46 tracks, TrackID #2 has ever been listened by a total of 3206 unique customers, so TrackID #2 ranks the first which is rank #0 in all of the 46 tracks in CustID #148.\n5. rankui = 0% would mean that program i is predicted to be the most desirable for user u, thus preceding all other programs in the list. On the other hand, rankui = 100% indicates that program i is predicted to be the least preferred for user u, thus placed at the end of the list."],"metadata":{}},{"cell_type":"code","source":["%sql \nselect CustId, count(TrackID)-1\nfrom predictions\ngroup by(CustId)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["%sql \nselect CustId, TrackID, rating, prediction,\n      (rank() over (partition by CustId order by prediction desc)-1) as numerator\nfrom predictions\nwhere CustId = 148"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["EPR_evaluation=spark.sql(\"\"\"\n      select CustId, TrackID, rating, \n            (rank() over (partition by CustId order by prediction desc)-1)*1.0/(count(TrackID) over (partition by CustId)-1) as p_rank\n      from predictions\n                        \"\"\")\nEPR_evaluation.createOrReplaceTempView(\"EPR_evaluation\")\n"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["display(EPR_evaluation)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["%sql\nselect CustID,sum(p_rank * rating) / sum(rating) as EPR_Customer\nfrom EPR_evaluation\ngroup by CustID\norder by EPR_Customer"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["result = spark.sql(\"\"\"\n      select sum(p_rank*rating)/sum(rating) as p_EPR\n      from EPR_evaluation\n    \"\"\")  "],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["display(result)"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["print(str(round(result.collect()[0][0]*100,2))+\"%\")"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["-sandbox\n\nEPR >= 50% indicates an algorithm no better than random."],"metadata":{}},{"cell_type":"code","source":["%sql\n--Benchmark: best EPR possible\n select sum(p_rank*rating)/sum(rating) as Best_EPR\n from (\n       select CustId, TrackID, rating, \n              (rank() over (partition by CustId order by rating desc)-1)*1.0/(count(TrackID) over (partition by CustId)-1) as p_rank\n       from rating_im_test\n       )\n    \n"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["\nfrom pyspark.ml.evaluation import Evaluator\n\nclass eprEvaluator(Evaluator):    \n  def _evaluate(self, predictions):\n    predictions.createOrReplaceTempView(\"predictions\")\n    result = spark.sql(\"\"\"\n      select sum(p_rank*rating)/sum(rating) as p_EPR\n      from (\n         select CustId, TrackID, rating, \n               (rank() over (partition by CustId order by prediction desc)-1)*1.0/(count(TrackID) over (partition by CustId)-1) as p_rank\n         from predictions\n          )\n    \"\"\").collect()[0][0]    \n    return float(result)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nals_im= ALS(alpha=30, maxIter=5, rank=50, regParam=0.1, userCol=\"CustId\", itemCol=\"TrackID\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im = als_im.fit(rating_im_train)\n\npredictions_im = model_im.transform(rating_im_test)\n\nepr_evaluator = eprEvaluator()\nepr_im = epr_evaluator.evaluate(predictions_im)\nprint(\"Expected percentile ranking for implicit rating = \" + str(epr_im))"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nals_im= ALS(alpha=200, maxIter=7, rank=50, regParam=0.08, userCol=\"CustId\", itemCol=\"TrackID\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im = als_im.fit(rating_im_train)\n\npredictions_im = model_im.transform(rating_im_test)\n\nepr_evaluator = eprEvaluator()\nepr_im = epr_evaluator.evaluate(predictions_im)\nprint(\"Expected percentile ranking for implicit rating = \" + str(epr_im))"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["# Generate top 10 movie recommendations for each user\nuserRecs_im = model_im.recommendForAllUsers(10)\n# Generate top 10 user recommendations for each song\nsongRecs_im = model_im.recommendForAllItems(10)\n"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["display(userRecs_im)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["display(songRecs_im)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":["-sandbox\n## 5.2 Recommendation Model from Implicit Ratings: Randomly split the dataset"],"metadata":{}},{"cell_type":"code","source":["def pop_fit(ratings_mat):\n    ratings_mat.createOrReplaceTempView(\"ratings_mat\")\n    model = spark.sql(\"\"\"\n      select TrackID, count(*) as prediction\n      from rating_im_train\n      group by TrackID\n    \"\"\")   \n    return model\n      \ndef pop_transform(model, test):\n    model.createOrReplaceTempView(\"model\")\n    test.createOrReplaceTempView(\"test\")\n    predictions = spark.sql(\"\"\"\n      select t.*, m.prediction\n      from rating_im_test t left join\n           model m\n           on t.TrackId=m.TrackId\n    \"\"\")   \n    return predictions\n\ndef bestepr(test):\n    test.createOrReplaceTempView(\"test\")\n    result = spark.sql(\"\"\"\n      select sum(p_rank*rating)/sum(rating) as Best_EPR\n      from (\n         select CustId, TrackID, rating, \n               (rank() over (partition by CustId order by rating desc)-1)*1.0/(count(TrackID) over (partition by CustId)-1) as p_rank\n         from test\n          )\n    \"\"\").collect()[0][0]    \n    return float(result)"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["#set training data using week 40 to week 51\nrating_im=spark.sql(\"\"\"\n\n  select CustId, TrackID, sum(case when Length>0 then 1 else 0 end) as rating\n  from final_table\n  group by CustId, TrackID\n\n\"\"\")\n\n\n\n(rating_im_tr, rating_im_te) = rating_im.randomSplit([0.8, 0.2],seed=50)\n\nrating_im.createOrReplaceTempView(\"rating_im\")\nrating_im_tr.createOrReplaceTempView(\"rating_im_tr\")\nrating_im_te.createOrReplaceTempView(\"rating_im_te\")\n\n"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["\nmodel_pop=pop_fit(rating_im_tr)\npredictions_pop=pop_transform(model_pop, rating_im_te)\nepr_evaluator = eprEvaluator()\nepr_pop = epr_evaluator.evaluate(predictions_pop)\nprint(\"Expected percentile ranking for popularity recommendation = \" + str(epr_pop))\n"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["%sql\n--Benchmark: best EPR possible\n select sum(p_rank*rating)/sum(rating) as Best_EPR\n from (\n       select CustId, TrackID, rating, \n              (rank() over (partition by CustId order by rating desc)-1)*1.0/(count(TrackID) over (partition by CustId)-1) as p_rank\n       from rating_im_te\n       )"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["als_im= ALS(alpha=30, maxIter=5, rank=50, regParam=0.1, userCol=\"CustId\", itemCol=\"TrackID\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im_r = als_im.fit(rating_im_tr)\n\npredictions_im_r = model_im.transform(rating_im_te)\nepr_evaluator = eprEvaluator()\nepr_im_r = epr_evaluator.evaluate(predictions_im_r)\nprint(\"Expected percentile ranking for implicit rating - random split = \" + str(epr_im_r))"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["als_im= ALS(alpha=100, maxIter=7, rank=50, regParam=0.08, userCol=\"CustId\", itemCol=\"TrackID\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im_r = als_im.fit(rating_im_tr)\n\npredictions_im_r = model_im.transform(rating_im_te)\nepr_evaluator = eprEvaluator()\nepr_im_r = epr_evaluator.evaluate(predictions_im_r)\nprint(\"Expected percentile ranking for implicit rating - random split = \" + str(epr_im_r))"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":["-sandbox\n## 5.3 Recommendation Model From Explicit Rating"],"metadata":{}},{"cell_type":"code","source":["\nrating_ex=spark.sql(\"\"\"\n  \nselect CustId, TrackID, \n       case when sum(case when Length>=0 then 1 else 0 end)<2 then 0\n            when sum(case when Length>=0 then 1 else 0 end)<4 then 1\n            when sum(case when Length>=0 then 1 else 0 end)<7 then 2\n            else 3 end as rating\nfrom final_table\ngroup by CustId, TrackID\n\"\"\")\n\n\n# Randomly split the dataset to train:test as 0.8:0.2; random seed=20\n(rating_ex_train, rating_ex_test) = rating_ex.randomSplit([0.8, 0.2],seed=20)\nrating_ex.createOrReplaceTempView(\"rating_ex\")\nrating_ex_train.createOrReplaceTempView(\"rating_ex_train\")\nrating_ex_test.createOrReplaceTempView(\"rating_ex_test\")\n"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["display(rating_ex)"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["model_pop=pop_fit(rating_ex_train)\npredictions_pop=pop_transform(model_pop, rating_ex_test)\nepr_evaluator = eprEvaluator()\nepr_pop = epr_evaluator.evaluate(predictions_pop)\nprint(\"Expected percentile ranking for popularity recommendation = \" + str(epr_pop))\n\nbest_epr = bestepr(rating_ex_test)\nprint(\"The best percentile ranking possible =\" + str(best_epr))"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["als_ex = ALS(rank=50, maxIter=7, regParam=0.06, userCol=\"CustId\", itemCol=\"TrackID\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                                    implicitPrefs=False, nonnegative=False)\nmodel_ex = als_ex.fit(rating_ex_train)\n\npredictions_ex = model_ex.transform(rating_ex_test)\n\nrmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\nrmse_ex = rmse_evaluator.evaluate(predictions_ex)\nepr_evaluator = eprEvaluator()\nepr_ex = epr_evaluator.evaluate(predictions_ex)\n\nprint(\"RMSE for explicit rating = \" + str(rmse_ex))\nprint (\"Expected percentile ranking for explicit rating = \" + str(epr_ex))"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["als_ex = ALS(rank=100, maxIter=7, regParam=0.1, userCol=\"CustId\", itemCol=\"TrackID\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                                    implicitPrefs=False, nonnegative=False)\nmodel = als_ex.fit(rating_ex_train)\n\npredictions = model.transform(rating_ex_test)\n\nrmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\nrmse = rmse_evaluator.evaluate(predictions)\nepr_evaluator = eprEvaluator()\nepr = epr_evaluator.evaluate(predictions)\n\nprint(\"RMSE for explicit rating = \" + str(rmse))\nprint (\"Expected percentile ranking = \" + str(epr))"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["#Grid search with TestValidationSplit\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\nals_ex= ALS(userCol=\"CustId\", itemCol=\"TrackID\", ratingCol=\"rating\", implicitPrefs=False, coldStartStrategy=\"drop\", maxIter=1, rank=5)\ngrid=ParamGridBuilder().addGrid(als_ex.regParam, [0.03,0.06]).build()\n#.addGrid(als_ex.regParam, [0.03,0.06,0.09])\nrmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n                                predictionCol=\"prediction\")\n\ntvs = TrainValidationSplit(estimator=als_ex, estimatorParamMaps=grid, evaluator=rmse_evaluator,trainRatio=0.8,seed=100)\ntvsModel = tvs.fit(rating_ex_train)\npredictions = tvsModel.transform(rating_ex_test)\n\nbest_model= tvsModel.bestModel\nprint (\"The rank for best model is \" + str(best_model.rank))\nprint (\"The Max number of iteration for best model is \" + str(best_model._java_obj.parent().getMaxIter()))"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["# Generate top 10 movie recommendations for each user\nuserRecs_ex = model_ex.recommendForAllUsers(10)\n# Generate top 10 user recommendations for each song\nsongRecs_ex = model_ex.recommendForAllItems(10)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["display(userRecs_ex)"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["display(songRecs_ex)"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":121}],"metadata":{"name":"Spotify_RecommendationSystem","notebookId":3283764495853970},"nbformat":4,"nbformat_minor":0}
